{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Phoenix ML Platform Documentation","text":"<p>Welcome to the official technical documentation for the Phoenix ML Platform. This platform is a state-of-the-art, high-throughput, low-latency system designed for real-time machine learning inference at scale.</p> <p>Built with Domain-Driven Design (DDD) and SOLID principles, Phoenix ML provides a robust foundation for deploying, monitoring, and maintaining machine learning models in production environments.</p>"},{"location":"#technical-core-pillars","title":"Technical Core Pillars","text":""},{"location":"#1-architectural-integrity-ddd-solid","title":"1. Architectural Integrity (DDD &amp; SOLID)","text":"<p>The platform is partitioned into strictly decoupled layers. By using the Command Pattern in the Application layer and Dependency Inversion in the Infrastructure layer, we ensure that the core ML logic (Domain) remains pure, testable, and completely independent of external frameworks like FastAPI or ONNX Runtime.</p>"},{"location":"#2-real-time-observability-feedback-loops","title":"2. Real-time Observability &amp; Feedback Loops","text":"<p>Observability is baked into the core. Every inference request is tracked for latency, throughput, and confidence. Using Prometheus and Grafana, the system provides sub-second visibility into model health. This real-time telemetry allows for proactive identification of performance regressions.</p>"},{"location":"#3-autonomous-self-healing-ml-monitoring","title":"3. Autonomous Self-Healing (ML Monitoring)","text":"<p>Phoenix ML addresses the \"silent failure\" problem in ML. The system features an integrated background monitoring service that performs statistical tests (e.g., Kolmogorov-Smirnov) on production data streams. Upon detecting significant Data Drift, the system logs alerts and triggers simulated retraining workflows to maintain model accuracy.</p>"},{"location":"#4-enterprise-grade-model-management","title":"4. Enterprise-Grade Model Management","text":"<p>With native support for ONNX Runtime, the platform handles models from various frameworks (PyTorch, Scikit-Learn, etc.) with unified high-performance execution. It supports sophisticated rollout strategies including A/B Testing and Canary Deployments via a dynamic model routing engine.</p>"},{"location":"#navigation-map","title":"Navigation Map","text":""},{"location":"#architecture-deep-dive","title":"\ud83c\udfd7\ufe0f Architecture &amp; Deep-Dive","text":"<ul> <li>System Design &amp; Data Flow: Detailed technical specifications, Sequence Diagrams, and ER Diagrams.</li> <li>Frontend Architecture: Breakdown of the React + TypeScript dashboard and state management strategy.</li> </ul>"},{"location":"#api-integration","title":"\ud83d\udd0c API &amp; Integration","text":"<ul> <li>API Reference: Exhaustive documentation of REST endpoints, JSON schemas, and error handling.</li> <li>Deployment Guide: Instructions for orchestrating the full stack (Kafka, Redis, Postgres, Monitoring).</li> </ul>"},{"location":"#architecture-decision-records-adr","title":"\ud83d\udcdc Architecture Decision Records (ADR)","text":"<p>These documents explain the rationale behind our critical technology choices: *   ADR 001: Adoption of DDD Architecture *   ADR 002: Standardization on ONNX Runtime *   ADR 003: Kafka for Real-time Event Streaming *   ADR 004: Metrics-Driven Observability Strategy</p> <p>Author: V\u00f5 Th\u00e0nh Nguy\u1ec5n Senior ML Platform Engineer Repository: phoenix_ML</p>"},{"location":"adr/001-use-ddd-architecture/","title":"ADR 001: Adoption of Domain-Driven Design (DDD)","text":""},{"location":"adr/001-use-ddd-architecture/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/001-use-ddd-architecture/#context","title":"Context","text":"<p>Machine Learning systems often suffer from \"Glue Code\" anti-patterns where business logic (inference rules, drift thresholds) interacts directly with infrastructure (databases, APIs). This high coupling makes the system fragile, untestable, and difficult to adapt to new requirements (e.g., switching from REST to gRPC, or Redis to Cassandra).</p>"},{"location":"adr/001-use-ddd-architecture/#decision","title":"Decision","text":"<p>We adopted Domain-Driven Design (DDD) to structure the codebase into four distinct layers:</p> <ol> <li>Domain Layer (Core): Contains high-level rules, Entities (Model, Prediction), and Value Objects (ConfidenceScore). This layer has ZERO dependencies on external frameworks.</li> <li>Application Layer: Orchestrates use cases using the Command Pattern (e.g., <code>PredictHandler</code>, <code>MonitoringService</code>). It coordinates the Domain objects but does not contain business rules.</li> <li>Infrastructure Layer: Implements interfaces defined in the Domain (Adapters). Includes FastAPI, Redis, PostgreSQL, and ONNX Runtime configurations.</li> <li>Shared Kernel: Utilities and common types shared across layers.</li> </ol>"},{"location":"adr/001-use-ddd-architecture/#consequences","title":"Consequences","text":""},{"location":"adr/001-use-ddd-architecture/#positive","title":"Positive","text":"<ul> <li>Testability: Core logic can be unit-tested in isolation without mocking heavy infrastructure.</li> <li>Flexibility: Infrastructure components can be swapped (e.g., replacing Redis with Memcached) without touching the Domain logic.</li> <li>Maintainability: Clear boundaries prevent \"spaghetti code\" as the system grows.</li> </ul>"},{"location":"adr/001-use-ddd-architecture/#negative","title":"Negative","text":"<ul> <li>Complexity: Increased number of files and boilerplate code compared to a simple script-based approach.</li> <li>Learning Curve: Requires team familiarity with DDD concepts (Aggregates, Repositories, Services).</li> </ul>"},{"location":"adr/002-use-onnx-runtime/","title":"ADR 002: Standardization on ONNX Runtime","text":""},{"location":"adr/002-use-onnx-runtime/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/002-use-onnx-runtime/#context","title":"Context","text":"<p>Deploying ML models directly via heavy training frameworks (PyTorch, TensorFlow) leads to bloated Docker images (&gt;3GB), slow startup times, and suboptimal inference latency. We need a unified runtime that is lightweight, fast, and framework-agnostic.</p>"},{"location":"adr/002-use-onnx-runtime/#decision","title":"Decision","text":"<p>We chose ONNX (Open Neural Network Exchange) as the standard model format and ONNX Runtime as the execution engine.</p>"},{"location":"adr/002-use-onnx-runtime/#consequences","title":"Consequences","text":""},{"location":"adr/002-use-onnx-runtime/#positive","title":"Positive","text":"<ul> <li>Performance: ONNX Runtime offers graph optimizations (operator fusion, constant folding) yielding 2x-10x speedups over native frameworks.</li> <li>Interoperability: Supports models trained in PyTorch, TensorFlow, Scikit-Learn, and XGBoost.</li> <li>Efficiency: Significantly reduced container size and memory footprint.</li> </ul>"},{"location":"adr/002-use-onnx-runtime/#negative","title":"Negative","text":"<ul> <li>Pipeline Complexity: Adds an extra \"export/conversion\" step in the MLOps pipeline.</li> <li>Operator Support: Some cutting-edge or custom layers in PyTorch might not yet be supported by the ONNX standard.</li> </ul>"},{"location":"adr/003-use-kafka-for-event-streaming/","title":"ADR 003: Asynchronous Event Streaming with Apache Kafka","text":""},{"location":"adr/003-use-kafka-for-event-streaming/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/003-use-kafka-for-event-streaming/#context","title":"Context","text":"<p>A high-throughput inference platform requires zero-latency impact from logging and monitoring tasks. In the initial prototype, synchronous database writes within the FastAPI request cycle increased p99 latency by &gt;20ms. Furthermore, multiple independent services (Drift Monitoring, Historical Audit, and BI Dashboards) require access to the same inference stream. </p> <p>We need a distributed, durable, and highly available event bus to decouple the synchronous Inference Pipeline from the asynchronous Observability Pipeline.</p>"},{"location":"adr/003-use-kafka-for-event-streaming/#decision","title":"Decision","text":"<p>We have standardized on Apache Kafka as the central event bus for all inference-related events. </p>"},{"location":"adr/003-use-kafka-for-event-streaming/#implementation-details","title":"Implementation Details:","text":"<ul> <li>Topic Design: A primary topic <code>inference-events</code> handles the fire-and-forget stream from the API.</li> <li>Protocol: We utilize <code>aiokafka</code> for non-blocking asynchronous production within Python.</li> <li>Architecture: To minimize operational complexity, we utilize Kafka KRaft Mode (Metadata Quorum), which removes the dependency on Zookeeper.</li> </ul>"},{"location":"adr/003-use-kafka-for-event-streaming/#consequences","title":"Consequences","text":""},{"location":"adr/003-use-kafka-for-event-streaming/#positive","title":"Positive","text":"<ul> <li>Inference Latency Reduction: API response time no longer depends on disk I/O or database performance.</li> <li>Durability: Kafka's append-only log ensures that events are safely stored even if the PostgreSQL database or the Drift Detector service is temporarily unavailable.</li> <li>Extensibility: We can spin up new consumer groups (e.g., for real-time Fraud Detection) without adding any load to the primary Inference API.</li> <li>Throughput: Naturally scales to handle millions of inference requests per day through horizontal partitioning.</li> </ul>"},{"location":"adr/003-use-kafka-for-event-streaming/#negative","title":"Negative","text":"<ul> <li>Operational Overhead: Requires monitoring Kafka cluster health (ISR, partition offsets, disk usage).</li> <li>Eventual Consistency: The monitoring dashboard and logs might lag by a few milliseconds/seconds behind the actual inference.</li> <li>Payload Management: Large feature vectors could potentially bloat Kafka topics if not managed (mitigated by only logging compressed or sampled data if necessary).</li> </ul>"},{"location":"adr/003-use-kafka-for-event-streaming/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>RabbitMQ: Rejected due to lower throughput for large log volumes and lack of persistent replay capabilities.</li> <li>Redis Pub/Sub: Rejected due to the lack of message persistence (data is lost if consumers are offline).</li> <li>AWS Kinesis: Rejected to maintain cloud-agnostic deployment via Docker/Kubernetes.</li> </ul>"},{"location":"adr/004-observability-with-prometheus-grafana/","title":"ADR 004: Unified Observability with Prometheus and Grafana","text":""},{"location":"adr/004-observability-with-prometheus-grafana/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/004-observability-with-prometheus-grafana/#context","title":"Context","text":"<p>A critical challenge in MLOps is the detection of \"Silent Failures\". Unlike traditional software where failures are binary (up/down), ML models can silently degrade in quality (Data Drift) while still returning successful HTTP 200 responses. We need a unified system to monitor: 1.  System Health: Latency, CPU/Memory usage, and Throughput. 2.  Model Health: Confidence distributions and Prediction outcomes. 3.  Statistical Health: Feature distribution shifts over time.</p>"},{"location":"adr/004-observability-with-prometheus-grafana/#decision","title":"Decision","text":"<p>We have implemented a comprehensive observability stack based on Prometheus (Metrics Aggregator) and Grafana (Visualization). </p>"},{"location":"adr/004-observability-with-prometheus-grafana/#key-design-elements","title":"Key Design Elements:","text":"<ul> <li>Instrumentation: Custom metrics are exported via the <code>/metrics</code> endpoint using the Prometheus ASGI middleware.</li> <li>Metric Types:<ul> <li><code>prediction_count_total</code> (Counter): Track throughput across different model versions (v1 vs v2).</li> <li><code>inference_latency_seconds</code> (Histogram): Track p50, p95, and p99 latencies to ensure SLA compliance.</li> <li><code>feature_drift_score</code> (Gauge): Real-time output from the <code>MonitoringService</code> statistical tests.</li> </ul> </li> <li>Dashboard-as-Code: Grafana is configured using Provisioning files. All dashboards and data sources are stored in Git (<code>grafana/provisioning/</code>), ensuring the monitoring environment is reproducible and version-controlled.</li> </ul>"},{"location":"adr/004-observability-with-prometheus-grafana/#consequences","title":"Consequences","text":""},{"location":"adr/004-observability-with-prometheus-grafana/#positive","title":"Positive","text":"<ul> <li>Sub-second Visibility: Real-time insight into the behavior of models under production load.</li> <li>A/B Test Monitoring: Instant visual comparison between Champion and Challenger models.</li> <li>Proactive Alerting: Enables the creation of alerts based on statistical drift thresholds before users notice accuracy drops.</li> <li>Portability: The entire stack can be moved from Docker Compose to Kubernetes (Prometheus Operator) with minimal configuration changes.</li> </ul>"},{"location":"adr/004-observability-with-prometheus-grafana/#negative","title":"Negative","text":"<ul> <li>Instrumentation Burden: Developers must manually add metrics logic to new service handlers.</li> <li>Storage Management: High-cardinality metrics (e.g., tracking metrics per unique <code>entity_id</code>) can bloat Prometheus storage. We mitigate this by only tracking aggregate-level metrics.</li> </ul>"},{"location":"adr/004-observability-with-prometheus-grafana/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>ELK Stack (Elasticsearch, Logstash, Kibana): Rejected as the primary monitoring tool because log-based monitoring is more expensive and slower than metric-based monitoring for real-time latency/throughput tracking.</li> <li>CloudWatch: Rejected to avoid cloud vendor lock-in and to maintain a local-first development experience.</li> </ul>"},{"location":"api/reference/","title":"API Reference: Phoenix ML Platform","text":"<p>The Phoenix ML API provides a high-performance REST interface for real-time inference and system monitoring.</p>"},{"location":"api/reference/#base-url","title":"Base URL","text":"<ul> <li>Local: <code>http://localhost:8000</code></li> <li>Production: Defined by your ingress controller (e.g., <code>https://ml.phoenix.com</code>)</li> </ul>"},{"location":"api/reference/#1-real-time-inference","title":"1. Real-time Inference","text":""},{"location":"api/reference/#post-predict","title":"<code>POST /predict</code>","text":"<p>Executes a machine learning model prediction. The system will automatically route to the best model version unless a specific version is requested.</p>"},{"location":"api/reference/#request-body-json","title":"Request Body (JSON)","text":"Field Type Required Description <code>model_id</code> string Yes The name of the model group (e.g., <code>credit-risk</code>). <code>model_version</code> string No Specific version (e.g., <code>v1</code>). Defaults to dynamic A/B routing. <code>entity_id</code> string No ID to fetch features from the Feature Store (Redis). <code>features</code> array[float] No Raw feature vector. Overrides Feature Store if provided."},{"location":"api/reference/#example-request","title":"Example Request","text":"<pre><code>{\n  \"model_id\": \"credit-risk\",\n  \"entity_id\": \"customer-good\"\n}\n</code></pre>"},{"location":"api/reference/#response-200-ok","title":"Response (200 OK)","text":"Field Type Description <code>model_id</code> string The model ID used for inference. <code>version</code> string The assigned version (determined by router). <code>result</code> integer The prediction class (e.g., <code>0</code> or <code>1</code>). <code>confidence</code> float The confidence score (0.0 to 1.0). <code>latency_ms</code> float Server-side execution time in milliseconds."},{"location":"api/reference/#2-model-monitoring","title":"2. Model Monitoring","text":""},{"location":"api/reference/#get-monitoringdriftmodel_id","title":"<code>GET /monitoring/drift/{model_id}</code>","text":"<p>Triggers an on-demand statistical drift check for a specific model group.</p>"},{"location":"api/reference/#path-parameters","title":"Path Parameters","text":"<ul> <li><code>model_id</code> (string, required): The ID of the model to analyze.</li> </ul>"},{"location":"api/reference/#response-200-ok_1","title":"Response (200 OK)","text":"<pre><code>{\n  \"feature_name\": \"feature_0\",\n  \"drift_detected\": true,\n  \"p_value\": 0.00001,\n  \"statistic\": 0.875,\n  \"threshold\": 0.05\n}\n</code></pre>"},{"location":"api/reference/#3-system-observability","title":"3. System Observability","text":""},{"location":"api/reference/#get-metrics","title":"<code>GET /metrics</code>","text":"<p>Exposes system-wide Prometheus metrics. Used by the Prometheus scraper.</p>"},{"location":"api/reference/#get-health","title":"<code>GET /health</code>","text":"<p>Liveness and readiness probe for the service.</p>"},{"location":"api/reference/#error-handling","title":"Error Handling","text":"<p>All errors return a standard JSON object: <pre><code>{\n  \"detail\": \"Error message description\"\n}\n</code></pre></p>"},{"location":"api/reference/#common-http-status-codes","title":"Common HTTP Status Codes","text":"Code Name Scenario 404 Not Found Requested model or version is not registered or active. 422 Validation Error Input data does not match the expected schema (e.g., non-numeric features). 500 Internal Error Inference engine crash or database connection failure. 503 Service Unavailable Circuit breaker is open due to repeated upstream failures."},{"location":"architecture/system-design/","title":"Detailed System Architecture: Phoenix ML Platform","text":"<p>This document provides an exhaustive technical analysis of the Phoenix ML Platform's architecture, data flow, and component interactions.</p>"},{"location":"architecture/system-design/#1-architectural-philosophy","title":"1. Architectural Philosophy","text":"<p>The platform is built on the Clean Architecture paradigm, ensuring that high-level business rules (the Domain) are isolated from low-level implementation details (Infrastructure).</p>"},{"location":"architecture/system-design/#high-level-architecture-c4-level-2","title":"High-Level Architecture (C4 Level 2)","text":"<pre><code>graph TD\n    Client[External Client] --&gt;|HTTPS/JSON| LB[Load Balancer / Ingress]\n    LB --&gt;|Round Robin| API[Phoenix API Gateway]\n\n    subgraph \"Core Services (FastAPI)\"\n        API --&gt;|PredictCommand| InferenceSvc[Inference Handler]\n        API --&gt;|Query| MonitorSvc[Monitoring Service]\n    end\n\n    subgraph \"Real-time Data Plane\"\n        InferenceSvc --&gt;|MGET &lt; 5ms| Redis[(Redis Feature Store)]\n        InferenceSvc --&gt;|Execute| ONNX[ONNX Runtime Engine]\n    end\n\n    subgraph \"Event &amp; Persistence Plane\"\n        InferenceSvc -.-&gt;|Async Event| Kafka{Kafka Event Bus}\n        Kafka --&gt;|Topic: inference-events| LogWorker[Postgres Log Repository]\n        LogWorker --&gt;|SQL| DB[(PostgreSQL)]\n    end\n\n    subgraph \"Observability Stack\"\n        InferenceSvc --&gt;|Export| Prom[Prometheus]\n        MonitorSvc --&gt;|Export| Prom\n        Prom --&gt;|Visualize| Grafana[Grafana Dashboard]\n    end</code></pre>"},{"location":"architecture/system-design/#2-core-workflows","title":"2. Core Workflows","text":""},{"location":"architecture/system-design/#21-real-time-inference-pipeline-sequence-diagram","title":"2.1. Real-time Inference Pipeline (Sequence Diagram)","text":"<p>The following diagram illustrates the lifecycle of a prediction request, emphasizing the non-blocking nature of the system.</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant API as FastAPI\n    participant H as PredictHandler\n    participant R as Redis (Feature Store)\n    participant E as ONNX Engine\n    participant K as Kafka\n\n    C-&gt;&gt;API: POST /predict (entity_id=\"cust_99\")\n    API-&gt;&gt;H: Execute(PredictCommand)\n\n    Note over H,R: Parallel Execution via Asyncio\n    par Fetch Features\n        H-&gt;&gt;R: HMGET features:cust_99\n        R--&gt;&gt;H: [income, debt, age, hist]\n    and Model Resolution\n        H-&gt;&gt;H: RoutingStrategy.select_model()\n        Note right of H: ABTest: Assign v2 (Challenger)\n    end\n\n    H-&gt;&gt;E: Predict(features)\n    Note over E: Executed in asyncio.to_thread\n    E--&gt;&gt;H: Prediction(Result=1, Conf=0.87)\n\n    par Async Observability\n        H-&gt;&gt;K: Publish(\"inference-events\", event)\n        H-&gt;&gt;H: Update Prometheus Metrics\n    end\n\n    H--&gt;&gt;API: PredictionResponse\n    API--&gt;&gt;C: 200 OK {result: 1, confidence: 0.87}</code></pre>"},{"location":"architecture/system-design/#3-data-architecture","title":"3. Data Architecture","text":""},{"location":"architecture/system-design/#31-persistence-layer-er-diagram","title":"3.1. Persistence Layer (ER Diagram)","text":"<p>We maintain a strict schema for model metadata and historical inference logs to facilitate reproducible auditing and drift analysis.</p> <pre><code>erDiagram\n    MODELS {\n        string id PK \"Model Name (e.g. credit-risk)\"\n        string version PK \"Semantic Version\"\n        string uri \"Storage Path\"\n        string framework \"onnx | tensorrt\"\n        string stage \"dev | staging | prod\"\n        jsonb metadata \"Feature names, roles\"\n        timestamp created_at\n        boolean is_active\n    }\n\n    PREDICTION_LOGS {\n        uuid id PK\n        string model_id FK\n        string model_version FK\n        jsonb features \"The feature vector used\"\n        jsonb result \"Predicted class/value\"\n        float confidence\n        float latency_ms\n        timestamp created_at\n    }\n\n    MODELS ||--o{ PREDICTION_LOGS : \"monitors\"</code></pre>"},{"location":"architecture/system-design/#4-technical-deep-dive","title":"4. Technical Deep Dive","text":""},{"location":"architecture/system-design/#41-the-concurrency-model","title":"4.1. The Concurrency Model","text":"<p>To maintain sub-50ms p99 latency, Phoenix ML utilizes a hybrid concurrency approach: -   I/O Bound: Redis and Postgres interactions use <code>async/await</code> with <code>asyncio</code>, allowing thousands of concurrent connections. -   CPU Bound: Inference via ONNX Runtime is compute-heavy and can block the Python event loop (GIL issues). We offload these calls to a dedicated thread pool using <code>asyncio.to_thread</code>, ensuring the API remains responsive.</p>"},{"location":"architecture/system-design/#42-model-routing-ab-testing","title":"4.2. Model Routing &amp; A/B Testing","text":"<p>The <code>PredictHandler</code> leverages the Strategy Pattern. When a request arrives without a specific version: 1.  It queries the Registry for all <code>is_active</code> versions of the model. 2.  It applies the configured <code>RoutingStrategy</code> (e.g., <code>ABTestStrategy</code> with a 50/50 split). 3.  The routing decision is logged in the <code>prediction_logs</code>, enabling comparison of Champion vs. Challenger performance in Grafana.</p>"},{"location":"architecture/system-design/#43-self-healing-via-drift-detection","title":"4.3. Self-Healing via Drift Detection","text":"<p>The \"Self-Healing\" capability is implemented as a continuous loop: -   Statistical Engine: Uses the Kolmogorov-Smirnov (KS) test to detect if the distribution of production features deviates significantly from the training baseline. -   Automatic Retraining (Trigger): Upon detecting drift (p-value &lt; 0.05), the <code>MonitoringService</code> issues a system-wide alert. In a full production environment, this event is published to Kafka to trigger an external MLOps pipeline (e.g., Airflow or Kubeflow).</p>"},{"location":"architecture/system-design/#5-error-handling-resiliency","title":"5. Error Handling &amp; Resiliency","text":"Failure Scenario Mitigation Strategy Result Redis Down Fallback to Raw Features in Request System remains functional but slower for clients. Kafka Down Retry with Exponential Backoff Logs are buffered in memory; no data loss for short outages. Model Load Fail ModelRegistry Validation Returns 404/500 with detailed diagnostic info. GPU Out-of-Memory CPU Fallback in ONNX Runtime Performance degrades but service stays up. <p>Document Status: Verified v1.0 Author: V\u00f5 Th\u00e0nh Nguy\u1ec5n</p>"},{"location":"deployment/docker-stack/","title":"Deployment Guide: Production Orchestration","text":"<p>Phoenix ML is designed to be cloud-agnostic and container-first. This guide covers the deployment of the full stack using Docker Compose and provides a roadmap for Kubernetes migration.</p>"},{"location":"deployment/docker-stack/#1-stack-architecture","title":"1. Stack Architecture","text":"<p>The platform consists of seven interconnected services, ensuring high availability and separation of concerns.</p> Service Technology Role api FastAPI + ONNX Runtime Core Inference Engine frontend React + Vite Control Plane Dashboard kafka Bitnami Kafka (KRaft) Asynchronous Event Streaming redis Redis 7 High-speed Online Feature Store db PostgreSQL 15 Model Metadata &amp; Audit Logs prometheus Prometheus 2 Metrics Collection grafana Grafana 9 Dashboard Visualization"},{"location":"deployment/docker-stack/#2-infrastructure-requirements","title":"2. Infrastructure Requirements","text":""},{"location":"deployment/docker-stack/#recommended-specs-localdev","title":"Recommended Specs (Local/Dev)","text":"<ul> <li>CPU: 4+ Cores (Inference is compute-intensive).</li> <li>RAM: 8GB Minimum (16GB recommended for full stack).</li> <li>Storage: 10GB SSD (For Kafka logs and Model artifacts).</li> </ul>"},{"location":"deployment/docker-stack/#gpu-acceleration","title":"GPU Acceleration","text":"<p>To enable NVIDIA GPU acceleration for ONNX Runtime: 1.  Ensure <code>nvidia-docker2</code> is installed on the host. 2.  Update <code>Dockerfile</code> to use <code>onnxruntime-gpu</code>. 3.  Add the <code>deploy.resources.reservations.devices</code> section to <code>api</code> in <code>compose.yaml</code>.</p>"},{"location":"deployment/docker-stack/#3-configuration-security","title":"3. Configuration &amp; Security","text":""},{"location":"deployment/docker-stack/#secure-secrets-management","title":"Secure Secrets Management","text":"<p>Do not store sensitive passwords in <code>compose.yaml</code> for production. Use a <code>.env</code> file (ignored by Git) or a secrets manager: <pre><code># Example .env entry\nDATABASE_URL=postgresql+asyncpg://admin:SECURE_PASSWORD@db:5432/phoenix\n</code></pre></p>"},{"location":"deployment/docker-stack/#persistence-strategy","title":"Persistence Strategy","text":"<p>The following volumes are defined to ensure zero data loss: -   <code>models:/app/models</code>: Stores the downloaded ONNX artifacts. -   <code>postgres_data:/var/lib/postgresql/data</code>: Stores metadata and logs. -   <code>grafana_data:/var/lib/grafana</code>: Stores dashboard customizations.</p>"},{"location":"deployment/docker-stack/#4-scaling-production-roadmap","title":"4. Scaling &amp; Production Roadmap","text":""},{"location":"deployment/docker-stack/#vertical-vs-horizontal-scaling","title":"Vertical vs. Horizontal Scaling","text":"<ul> <li>API Horizontal Scaling: Increase <code>api</code> replicas to handle more RPS.     <pre><code>docker compose up -d --scale api=5\n</code></pre></li> <li>Inference Vertical Scaling: Move <code>api</code> to instances with higher memory bandwidth or GPU support.</li> </ul>"},{"location":"deployment/docker-stack/#kubernetes-migration-production-target","title":"Kubernetes Migration (Production Target)","text":"<p>For enterprise scale, migrate the stack to Kubernetes: 1.  Helm Charts: Create charts for the <code>api</code> and <code>frontend</code>. 2.  Managed Services: Replace self-hosted Kafka/Postgres with managed services (e.g., Confluent Cloud, AWS RDS). 3.  Ingress: Use an Nginx Ingress Controller or Istio for advanced traffic routing and rate limiting. 4.  HPA: Configure Horizontal Pod Autoscaler based on the <code>inference_latency_seconds</code> metric from Prometheus.</p>"},{"location":"frontend/architecture/","title":"Frontend Architecture: Phoenix Dashboard","text":"<p>The frontend is a single-page application (SPA) built to provide real-time visibility into the ML Platform.</p>"},{"location":"frontend/architecture/#tech-stack","title":"Tech Stack","text":"<ul> <li>Framework: React 18 + TypeScript (Vite build tool)</li> <li>Styling: Tailwind CSS (Utility-first architecture)</li> <li>State Management: TanStack Query (React Query) for server-state caching and revalidation.</li> <li>Icons: Lucide React.</li> </ul>"},{"location":"frontend/architecture/#component-design-solid","title":"Component Design (SOLID)","text":"<p>We adhere to the Single Responsibility Principle for UI components:</p> <ul> <li><code>StatCard</code>: Pure presentational component for displaying metrics.</li> <li><code>mlService</code>: Isolated API layer. UI components never call <code>fetch/axios</code> directly.</li> <li><code>App.tsx</code>: Orchestrator (Container) component that manages layout.</li> </ul>"},{"location":"frontend/architecture/#key-features","title":"Key Features","text":"<ol> <li>A/B Test Simulation:<ul> <li>Buttons to simulate \"Good\" vs \"Bad\" customer profiles.</li> <li>Visual feedback on Model Routing (Champion vs Challenger).</li> </ul> </li> <li>Drift Visualization:<ul> <li>Real-time display of Kolmogorov-Smirnov (KS) statistics.</li> <li>Visual alerts when drift exceeds thresholds.</li> </ul> </li> </ol>"},{"location":"frontend/architecture/#project-structure","title":"Project Structure","text":"<pre><code>frontend/src/\n\u251c\u2500\u2500 api/           # API integration (Axios)\n\u251c\u2500\u2500 types/         # TypeScript Interfaces (DRY)\n\u251c\u2500\u2500 App.tsx        # Main Dashboard Layout\n\u2514\u2500\u2500 main.tsx       # Entry point\n</code></pre>"}]}